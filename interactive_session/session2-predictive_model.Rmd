---
title: "Building Your Predictive Model"
author: "You!"
output: 
  html_document:
    toc: true
editor_options: 
  chunk_output_type: inline
---

In this R notebook, we're going to learn the basics of predictive modeling. 

# Before we get started

Save this under a different file name using File >> Save as. 



# Learning Objectives of this Notebook

1. **Understand** that model building involves selecting explanatory variables
2. **Understand** the impact of missing values on building a model.
3. **Build** a simple model to explain 30 day hospital readmissions
4. **Evaluate** the model in terms of our own priorities and values 
Note that the goal today is not to turn you into a statistical programmer. It's to introduce you to the basic concepts behind predicitve modeling.


# Tour of R/RStudio

[Panels]
[Environment]
[Files]



## Basic workflow for predictive modeling

This is an extremely simplified version of predictive modeling, but hopefully it will give you a sense for each step.


1. Clean Data using select/drop_na
2. Separate data into training/test sets
3. Train model using training set
4. Predict probabilities using test set
5. Pick cutoff and assess model performance




## First Things First

The first thing to do is to load the data into your workspace. Click the `play` button on the below code chunk. 

We first load in some packages: `broom`, `tidyr`, `dplyr`, and `visdat`, `janitor`. Then we're going to load our data using the `read_rds` function. We use the `<-` to *assign* our data into the `hosp_readmit_data` object. 

```{r setup}
library(broom)
library(tidyr)
library(dplyr)
library(visdat)
library(caret)
library(janitor)

hosp_readmit_data <- readRDS("data/dataset.rds")
```



## Show the first few rows of the data

Let's look at the first few rows of the data with the `head()` command. The head command will show the first few rows.

```{r}
head(hosp_readmit_data)
```

Remember, if you want to see the full table, you can use the `View()` command:

```{r}
View(hosp_readmit_data)
```



## Data Wrangling 101: Select our covariates

Ok, now we're going to build a simple predictive model with our covariates. We're going to use the `select` function in the `dplyr` package to pick our variables from the larger dataset. This is important to do beforehand because we're going to select the complete cases in our data to model (see below).

This will seem weird at first, but the `%>%` is what's called a `pipe` and lets us flow our data from one function to another. When I read my code out loud, I usually read it as "then".

For example, I would read the following statement as:

I took `hosp_readmit_data` and THEN
I only `select`ed the `readmit30`, `age`, `length_of_stay` columns.

Think about it: which variables are we selecting? What is our outcome we're trying to predict?


```{r}
hosp_readmit_data_model <- hosp_readmit_data %>% 
  select(readmit30, age, length_of_stay)

head(hosp_readmit_data_model)
```

We're going to use `visdat` to summarize our data again. What do you notice in our dataset?

```{r}
visdat::vis_dat(hosp_readmit_data_model)
```



## What are you going to do about `NA`s?

Ugh. There are NAs (missing values) in our data! We're going to use the `drop_na` function to remove all rows that are not complete. 

```{r}
hosp_readmit_data_model_filtered <- hosp_readmit_data_model %>% tidyr::drop_na()

visdat::vis_dat(hosp_readmit_data_model_filtered)
```

```{r}
nrow(hosp_readmit_data_model)
```

# How many patients did we lose?

```{r}
nrow(hosp_readmit_data_model_filtered)
```



## Separating out our data

Ok, now we have to separate our data into two sets: the *training* set and the *test* set. 

1. Training set: a set of data with which we build (or train) our model with. 
2. Test set: a set of data with which we test the predictive power of our model.

How do we relate the test/train set to internal validity? Why is it important to hold out some data for testing?

```{r}
train_index <- createDataPartition(hosp_readmit_data_model_filtered$readmit30, p=0.85, list=FALSE)

train_data <- hosp_readmit_data_model_filtered[train_index,]
test_data <- hosp_readmit_data_model_filtered[-train_index,]

```

Show the number of rows in our training data:

```{r}
nrow(train_data)
```

Show the number of rows in our test data:

```{r}
nrow(test_data)
```



## A Basic Model

Here we're going to build a basic model with `any_cvd` as our outcome (what we want to predict), and `shhs_train_data` as our data. 

Take a look at how we build the model below. The first thing we need to specify is our *forumla*. 

One of the most confusing things about R is the formula interface. The thing to remember is that formulas have a certain form. If `Y` is our dependent variable and `X1`, `X2` are independent variables, then the formula to predict `Y` has the format `Y ~ X1 + X2`. 

Usually these variables come from a data.frame, which is supplied by the `data` argument to the function. Note that we don't need quotes to refer to the variables in the `data.frame`.

```{r}
basic_model <- glm(#put your formula below
                   formula = readmit30 ~ age + length_of_stay, 
                   # binomial
                   family = "binomial", 
                   #we put our data into the data argument
                   data =    shhs_train_data
                   )
```

## Predictive model

The important thing to understand with logistic regression is that it actually calculates a probability, which is the likelihood that you are likely to have cardiovascular disease in the next 10 years. A probablility of 0.9 means that you are more likely to have CVD, and a probability of 0.1 means that you are less likely to have CVD.

Let's plug in a couple of patients into our model. 

1. `patient1` is going to be older `65`, but have a shorter `length_of_stay` (`5` days). Is this patient more likely to be readmitted or not?

2. `patient2` is going to be younger `25`, and has a longer `length_of_stay` (`23` days). Is this patient more likely to be readmitted or not?

3. `patient3` is a middle aged (`42`) patient,  and has shorter `length_of_stay` (`10` days)? Is this patient more likely to be readmitted or not?

4. `patient41` is an older (`60`) patient, a long `length_of_stay` (`41` days). What do you think?

Let's plug in these four patients into our model. First we specify our data. We have to specify each variable separately and then glue them together as a `data.frame`.

```{r}
pat_name <- c("patient1", "patient2", "patient3", "patient4")
age <- c(65, 25, 42, 60)
length_of_stay <- c(5, 23, 10, 41)

patient_table <- data.frame(pat_name, age, length_of_stay)
patient_table
```

We can pass `patient_table` into our model with the `augment` function and it will evaluate our patients. When we look at this table, we can see that the `.fitted` column contains our predicted probabilities.

```{r}
pat_table_aug <- augment(basic_model, newdata=patient_table, type.predict = "response")
pat_table_aug
```

Let's plot the predicted probability for each patient.

```{r}
pat_table_aug %>% ggplot(aes(x=pat_name, y=.fitted, fill=pat_name)) + 
  geom_bar(stat="identity") + ggtitle("Predicted probability for each patient")
```

One thing to note: even though we though patient 1 had a high probability of CVD, they are only predicted to have a 40% probability of having CVD. This suggests that our model is not completely predicting with 100% certainty. This is to be expected, because there are many other types of data we are not capturing that may explain the remainder of the probability. 

One such data point might be hours of exercise a week, and another might be diet.

Also, patient 3, which we thought might be on the cusp of the predicted probability, actually has a low predicted probability! What about patient 4?



The predictor variables aren't all equally important in the model. They're actually weighted in terms of importance. We can see this if we use `tidy` on `basic_model`.

```{r}
tidy(basic_model)
```

The coefficients of our model specify the weights, or importance of our variables in calculating the predicted probablity. Another thing to note is the `p.value` associated with each variable in our model. For an alpha cutoff of 0.05, all three variables are highly significant predictors in the model.




## Evaluating using our test set

What if we plug in our test set into the model? How are the predicted probabilities distributed?

```{r}
predictions <- augment(basic_model, newdata = shhs_test_data, type.predict = "response")
predictions
```

Plot the histogram of probabilities:

```{r}
predictions %>% ggplot(aes(x=.fitted)) + geom_histogram()
```

We see that the majority of our test patients have a lower predicted probability. Naively, let's choose that if our patient has a predicted probability > 0.5, that they are a cardiovascular risk and if they are less than or equal to 0.5, they are not a cardiovascular risk. Let's recode a new variable, `predict_cvd`, with this variable

```{r}
cutoff <- 0.5

predictions2 <- predictions %>% mutate(predict_readmit30 = case_when(.fitted > cutoff ~ "Yes", 
                                               .fitted <= cutoff ~ "No"))

predictions2
```

Now we have a set of predictions and we can compare them to the true value `any_cvd` in our dataset. 

```{r}
conf_matrix <- predictions2 %>% tabyl(readmit30, predict_readmit30)
conf_matrix
```

Try adjusting the `cutoff` above and see how the different cells of the table change.




## Accuracy versus balanced accuracy

Try adjusting `cutoff` below and look at what happens to Accuracy versus Balanced accuracy.

```{r}
cutoff <- 0.3

#recode our .fitted into a predictiona by using cutoff
predictions2 <- predictions %>% 
  mutate(predict_readmit30 = case_when(.fitted > cutoff ~ 1, 
                                       .fitted <= cutoff ~ 0))

#make our predictions categorical by transforming them into factor
predictions2 <- predictions2 %>% 
  mutate(readmit30 = factor(readmit30), predict_readmit30 = factor(predict_readmit30))

#show metrics on our predictions
caret::confusionMatrix(predictions2$predict_readmit30, predictions$readmit30,  positive="1")
```


```{r}
tidy(basic_model)
```

# Review: Basic workflow for predictive modeling


1. Clean Data using select/drop_na
2. Separate data into training/test sets
3. Train model using training set
4. Predict probabilities using test set
5. Pick cutoff and assess model performance




# Your Model:

You have selected a set of covariates as predictors of your model. Input them in the code below.

```{r}
#remember to subset the proper variables!
hosp_readmit_data_filtered2 <- hosp_readmit_data %>% 
  select(#fill in with the variables! 
    ) %>% 
#use drop_na() for now  
  drop_na()

#separate into test/train sets
#remember, we're just reusing the partitions from above
train_data2 <- hosp_readmit_data_filtered2[train_index,]
test_data2 <- hosp_readmit_data_filtered2[-train_index,]
```

# Run Model

Put in your model in the form of (readmit30 ~ age + length_of_stay)

```{r}
basic_model2 <- glm(readmit30 ~  ,#fill your covariates here
                    family = "binomial", 
                    #we put our data into the data argument
                    data =    train_data2
)
```

# Predict on Training Set

```{r}
#evaluate basic_model2 here
predictions <- augment(basic_model2, newdata = test_data2, type.predict = "response")

predictions %>% ggplot(aes(x=.fitted)) + geom_histogram()
```

## Decide on a Cutoff and predict

```{r}
cutoff <- 0.5

predictions2 <- predictions %>% 
  mutate(predict_readmit30 = 
           case_when(.fitted > cutoff ~ 1, 
                     .fitted <= cutoff ~ 0))

caret::confusionMatrix( predictions2$predict_readmit30, predictions$readmit30,  positive="1")
```


## How did you do?

Post in chat your model, cutoff, and sensitivity/specificity

How do you intepret the model for your patient population? Is the data adequate to predict patients who will be readmitted?



# Summary

We learned a lot today in this notebook! Specifically,

1. How to select our variables using `select()`
2. How to only use complete cases using `drop_na()`
3. How to separate our data into test/train sets
4. How to build our model using `glm()`
5. Testing the predictive power of our model using patients
5. Ways to evaluate the predictive power of our model

